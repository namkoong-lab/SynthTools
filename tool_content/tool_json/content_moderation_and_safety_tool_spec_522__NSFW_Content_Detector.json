{
  "tool_name": "NSFW Content Detector",
  "tool_description": "Detects explicit sexual content, nudity, and adult material in images using deep learning models.",
  "parameters": {
    "image_path": {
      "type": "string",
      "required": true,
      "description": "Path to the preprocessed image file"
    },
    "sensitivity_level": {
      "type": "string",
      "required": false,
      "description": "Detection sensitivity: low, medium, high",
      "default": "medium"
    },
    "confidence_threshold": {
      "type": "number",
      "required": false,
      "description": "Minimum confidence score (0.0-1.0) to flag content",
      "default": 0.7
    }
  },
  "error_messages": [
    "Image not accessible: Cannot read the image file. Ensure the file exists and is properly preprocessed.",
    "Invalid sensitivity: sensitivity_level must be one of: low, medium, high.",
    "Invalid threshold: confidence_threshold must be between 0.0 and 1.0.",
    "Analysis failed: Unable to analyze image. File may be corrupted or in wrong format."
  ],
  "usage": "Provide image_path to a preprocessed image. Adjust sensitivity_level and confidence_threshold based on your moderation policies. Higher sensitivity detects more potential issues but may increase false positives.",
  "output_details": {
    "is_nsfw": {
      "type": "boolean",
      "description": "Whether the image contains NSFW content above threshold"
    },
    "nsfw_score": {
      "type": "number",
      "description": "Confidence score for NSFW content (0.0-1.0)"
    },
    "content_categories": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Detected NSFW categories like nudity, sexual, suggestive"
    },
    "risk_level": {
      "type": "string",
      "description": "Overall risk assessment: low, medium, high, critical"
    }
  }
}