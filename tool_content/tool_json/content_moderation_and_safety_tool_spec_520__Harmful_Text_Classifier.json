{
  "tool_name": "Harmful Text Classifier",
  "tool_description": "Analyzes extracted text content for hate speech, harassment, threats, and other harmful language using natural language processing models.",
  "parameters": {
    "text_content": {
      "type": "string",
      "required": true,
      "description": "Text content to analyze for harmful language"
    },
    "classification_types": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "required": false,
      "description": "Types to classify: 'hate_speech', 'harassment', 'threats', 'profanity', 'spam'",
      "default": [
        "hate_speech",
        "harassment",
        "threats",
        "profanity"
      ]
    },
    "language": {
      "type": "string",
      "required": false,
      "description": "Text language for analysis (ISO 639-1 code)",
      "default": "en"
    },
    "severity_threshold": {
      "type": "number",
      "required": false,
      "description": "Minimum severity score for flagging (0.1-1.0)",
      "default": 0.5
    }
  },
  "error_messages": [
    "Empty text input: Provide non-empty text_content for analysis.",
    "Invalid classification types: Use valid types from ['hate_speech', 'harassment', 'threats', 'profanity', 'spam'].",
    "Unsupported language: The specified language code is not supported by the classifier.",
    "Invalid severity threshold: severity_threshold must be between 0.1 and 1.0.",
    "Classification failed: Text analysis engine encountered an error during processing."
  ],
  "usage": "Provide text_content extracted from media. Select classification_types to focus on specific harmful content categories. Set severity_threshold based on platform tolerance levels.",
  "output_details": {
    "is_harmful": {
      "type": "boolean",
      "description": "Whether harmful content was detected above threshold"
    },
    "classification_scores": {
      "type": "array",
      "items": {
        "type": "number"
      },
      "description": "Severity scores for each classification type"
    },
    "flagged_categories": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Categories where harmful content was detected"
    },
    "overall_toxicity": {
      "type": "number",
      "description": "Overall toxicity score (0.0-1.0)"
    }
  }
}