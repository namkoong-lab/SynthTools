{
  "tool_name": "Payment_Data_Tokenizer",
  "tool_description": "Tokenizes payment card data to replace sensitive cardholder information with non-sensitive tokens, reducing PCI DSS scope and protecting stored payment data.",
  "parameters": {
    "input_data_source": {
      "type": "string",
      "required": true,
      "description": "Path to data source containing payment card data to be tokenized."
    },
    "tokenization_method": {
      "type": "string",
      "required": true,
      "description": "Tokenization method: format-preserving, random, or vault-based."
    },
    "data_fields": {
      "type": "array",
      "required": true,
      "description": "List of data fields to tokenize (PAN, expiry-date, cardholder-name, etc.).",
      "items": {
        "type": "string"
      },
      "minItems": 1,
      "maxItems": 10
    },
    "preserve_format": {
      "type": "boolean",
      "required": false,
      "description": "Whether to preserve original data format in tokens.",
      "default": true
    },
    "token_length": {
      "type": "integer",
      "required": false,
      "description": "Length of generated tokens (8-32 characters).",
      "default": 16
    }
  },
  "error_messages": [
    "Data source not found: Verify the input_data_source path exists and contains valid payment data.",
    "Invalid tokenization method: Use 'format-preserving', 'random', or 'vault-based' for tokenization_method parameter.",
    "Invalid data fields: Specify valid payment data field names like 'PAN', 'expiry-date', 'cardholder-name'.",
    "Token length out of range: token_length must be between 8 and 32 characters.",
    "Tokenization failed: Unable to generate secure tokens for the provided payment data.",
    "Data format incompatible: The input data format is not compatible with the selected tokenization method."
  ],
  "usage": "Provide input data source path, select tokenization method, and specify which payment data fields to tokenize. Optionally configure format preservation and token length. The tool replaces sensitive payment data with secure tokens.",
  "output_details": {
    "tokenized_records": {
      "type": "integer",
      "description": "Number of payment records successfully tokenized"
    },
    "token_mapping_file": {
      "type": "string",
      "description": "Path to secure file containing token-to-data mappings"
    },
    "tokenization_algorithm": {
      "type": "string",
      "description": "Algorithm used for tokenization process"
    },
    "processing_timestamp": {
      "type": "string",
      "description": "Timestamp when tokenization was completed",
      "format": "date-time"
    },
    "scope_reduction_estimate": {
      "type": "string",
      "description": "Estimated reduction in PCI DSS compliance scope after tokenization"
    }
  }
}