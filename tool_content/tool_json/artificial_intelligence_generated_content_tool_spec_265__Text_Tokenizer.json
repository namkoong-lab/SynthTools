{
  "tool_name": "Text Tokenizer",
  "tool_description": "Breaks down input text into linguistic units (words, sentences, paragraphs) for analysis by other grammar checking tools.",
  "parameters": {
    "text": {
      "type": "string",
      "required": true,
      "description": "The input text to be tokenized"
    },
    "tokenization_level": {
      "type": "string",
      "required": false,
      "description": "Level of tokenization: word, sentence, or paragraph",
      "default": "word"
    }
  },
  "error_messages": [
    "Empty text input: Provide non-empty text string for tokenization.",
    "Invalid tokenization level: Use one of [word, sentence, paragraph].",
    "Text too long: Input text must be under 50,000 characters."
  ],
  "usage": "Provide the text to tokenize and optionally specify the tokenization level. Returns structured tokens with position information.",
  "output_details": {
    "tokens": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Array of text tokens based on specified level"
    },
    "token_positions": {
      "type": "array",
      "items": {
        "type": "integer"
      },
      "description": "Character positions where each token starts in original text"
    },
    "total_tokens": {
      "type": "integer",
      "description": "Total number of tokens identified"
    }
  }
}