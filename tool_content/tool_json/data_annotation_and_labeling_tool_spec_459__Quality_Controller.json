{
  "tool_name": "Quality Controller",
  "tool_description": "Performs comprehensive quality control on annotation batches, identifying inconsistencies, measuring inter-annotator agreement, and generating quality reports.",
  "parameters": {
    "project_id": {
      "type": "string",
      "required": true,
      "description": "Project identifier for quality control scope"
    },
    "annotation_batch_ids": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "required": true,
      "description": "List of annotation batch identifiers to analyze"
    },
    "quality_metrics": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "required": false,
      "default": [
        "consistency",
        "completeness",
        "accuracy",
        "inter_annotator_agreement"
      ],
      "description": "Quality metrics to compute"
    },
    "reference_annotator": {
      "type": "string",
      "required": false,
      "default": "",
      "description": "Reference annotator ID for accuracy comparison"
    },
    "agreement_threshold": {
      "type": "number",
      "required": false,
      "default": 0.8,
      "description": "IoU threshold for measuring annotation agreement (0.0 to 1.0)"
    }
  },
  "error_messages": [
    "Invalid project_id: Project identifier must exist and be non-empty. Verify project_id is correct.",
    "Empty annotation_batch_ids: At least one batch ID is required for quality control. Provide batch IDs in the array.",
    "Batch not found: One or more annotation_batch_ids do not exist in the project. Check all batch IDs are valid.",
    "Invalid quality_metrics: Metrics must be from [consistency, completeness, accuracy, inter_annotator_agreement]. Check metric names.",
    "Invalid reference_annotator: Reference annotator ID does not exist in the project. Verify annotator_id or leave empty.",
    "Invalid agreement_threshold: Threshold must be between 0.0 and 1.0 inclusive. Adjust agreement_threshold value."
  ],
  "usage": "Provide project_id and annotation_batch_ids for quality analysis. Select quality_metrics to compute and optionally specify reference_annotator and agreement_threshold. Returns comprehensive quality control report.",
  "output_details": {
    "overall_quality_score": {
      "type": "number",
      "description": "Overall quality score from 0.0 to 1.0"
    },
    "metric_scores": {
      "type": "array",
      "items": {
        "type": "number"
      },
      "description": "Individual scores for each requested metric"
    },
    "flagged_annotations": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "List of annotation IDs flagged for review"
    },
    "annotator_performance": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Performance summary for each annotator"
    },
    "quality_issues": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Identified quality issues and recommendations"
    },
    "report_timestamp": {
      "type": "string",
      "description": "When the quality control was performed"
    }
  }
}