{
  "tool_name": "Risk Evaluator",
  "tool_description": "Evaluates overall risk posed by content considering severity, user factors, platform context, and potential impact to determine appropriate response measures.",
  "parameters": {
    "severity_score": {
      "type": "number",
      "required": true,
      "description": "Content severity score from severity assessment (0.0-1.0)"
    },
    "confidence_level": {
      "type": "string",
      "required": true,
      "description": "Confidence in detection results: low, medium, high"
    },
    "user_history_risk": {
      "type": "string",
      "required": false,
      "description": "User's violation history: clean, minor_violations, repeat_offender, severe_violator",
      "default": "clean"
    },
    "audience_size": {
      "type": "integer",
      "required": false,
      "description": "Potential audience size for the content",
      "default": 1
    },
    "platform_visibility": {
      "type": "string",
      "required": false,
      "description": "Content visibility: private, limited, public, viral_potential",
      "default": "public"
    },
    "false_positive_probability": {
      "type": "number",
      "required": false,
      "description": "Probability of false positive (0.0-1.0)",
      "default": 0.0
    },
    "target_vulnerability": {
      "type": "string",
      "required": false,
      "description": "Vulnerability of target audience: general, minors, vulnerable_groups",
      "default": "general"
    },
    "escalation_potential": {
      "type": "boolean",
      "required": false,
      "description": "Whether content could lead to further escalation",
      "default": false
    },
    "legal_implications": {
      "type": "boolean",
      "required": false,
      "description": "Whether content may have legal implications",
      "default": false
    },
    "cultural_sensitivity": {
      "type": "string",
      "required": false,
      "description": "Cultural sensitivity level: low, medium, high",
      "default": "medium"
    },
    "time_sensitivity": {
      "type": "string",
      "required": false,
      "description": "Time sensitivity for response: low, medium, high, urgent",
      "default": "medium"
    }
  },
  "error_messages": [
    "Invalid severity score: Score must be between 0.0 and 1.0.",
    "Invalid confidence level: Use low, medium, or high.",
    "Invalid user history risk: Use clean, minor_violations, repeat_offender, or severe_violator.",
    "Invalid audience size: Must be positive integer.",
    "Invalid platform visibility: Use private, limited, public, or viral_potential.",
    "Invalid false positive probability: Must be between 0.0 and 1.0.",
    "Invalid target vulnerability: Use general, minors, or vulnerable_groups.",
    "Invalid cultural sensitivity: Use low, medium, or high.",
    "Invalid time sensitivity: Use low, medium, high, or urgent."
  ],
  "usage": "Provide severity_score and confidence_level as required inputs. Include user_history_risk, audience_size, and platform_visibility for comprehensive risk assessment. Set optional parameters based on specific context and requirements.",
  "output_details": {
    "overall_risk_level": {
      "type": "string",
      "description": "Overall risk classification: minimal, low, medium, high, critical"
    },
    "risk_score": {
      "type": "number",
      "description": "Numerical risk score (0.0-1.0)"
    },
    "recommended_actions": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "List of recommended moderation actions"
    },
    "urgency_level": {
      "type": "string",
      "description": "Response urgency: routine, priority, urgent, immediate"
    },
    "escalation_required": {
      "type": "boolean",
      "description": "Whether escalation to human moderators is required"
    }
  }
}