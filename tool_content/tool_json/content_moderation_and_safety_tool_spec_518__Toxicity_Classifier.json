{
  "tool_name": "Toxicity Classifier",
  "tool_description": "Classifies text content for various forms of toxic language including threats, insults, and aggressive behavior using machine learning models.",
  "parameters": {
    "text_content": {
      "type": "string",
      "required": true,
      "description": "Text content to analyze for toxicity"
    },
    "language_code": {
      "type": "string",
      "required": false,
      "description": "ISO 639-1 language code for language-specific analysis",
      "default": "en"
    },
    "toxicity_threshold": {
      "type": "number",
      "required": false,
      "description": "Threshold for toxicity classification (0.0-1.0)",
      "default": 0.7
    },
    "include_subcategories": {
      "type": "boolean",
      "required": false,
      "description": "Whether to include detailed toxicity subcategory scores",
      "default": true
    }
  },
  "error_messages": [
    "Empty text content: Provide non-empty text for toxicity analysis.",
    "Unsupported language: Language code not supported for toxicity detection.",
    "Invalid threshold: Toxicity threshold must be between 0.0 and 1.0.",
    "Text too long: Maximum supported text length is 10,000 characters."
  ],
  "usage": "Input text_content with optional language_code. Adjust toxicity_threshold to control sensitivity. Enable include_subcategories for detailed analysis.",
  "output_details": {
    "is_toxic": {
      "type": "boolean",
      "description": "Whether content is classified as toxic"
    },
    "toxicity_score": {
      "type": "number",
      "description": "Overall toxicity score (0.0-1.0)"
    },
    "threat_score": {
      "type": "number",
      "description": "Score for threatening language (0.0-1.0)"
    },
    "insult_score": {
      "type": "number",
      "description": "Score for insulting language (0.0-1.0)"
    },
    "identity_attack_score": {
      "type": "number",
      "description": "Score for identity-based attacks (0.0-1.0)"
    }
  }
}