{
  "tool_name": "Violence Detection Engine",
  "tool_description": "Analyzes images or video frames to detect violent content including fights, weapons, aggressive behavior, and threatening gestures using computer vision models.",
  "parameters": {
    "image_paths": {
      "type": "array",
      "required": true,
      "description": "Paths to image files to analyze",
      "items": {
        "type": "string"
      },
      "minItems": 1,
      "maxItems": 100
    },
    "detection_sensitivity": {
      "type": "string",
      "required": false,
      "description": "Sensitivity level: low, medium, or high",
      "default": "medium"
    },
    "violence_categories": {
      "type": "array",
      "required": false,
      "description": "Specific violence categories to detect",
      "items": {
        "type": "string"
      },
      "default": [
        "weapons",
        "fighting",
        "aggressive_behavior",
        "threatening_gestures"
      ]
    },
    "confidence_threshold": {
      "type": "number",
      "required": false,
      "description": "Minimum confidence score for positive detection (0.0-1.0)",
      "default": 0.7
    },
    "batch_processing": {
      "type": "boolean",
      "required": false,
      "description": "Enable batch processing for multiple images",
      "default": true
    }
  },
  "error_messages": [
    "Invalid image paths: One or more image files could not be found or read.",
    "Invalid sensitivity level: Use one of [low, medium, high].",
    "Invalid confidence threshold: Value must be between 0.0 and 1.0.",
    "Too many images: Maximum 100 images per batch.",
    "Detection model error: Violence detection model failed to process the images."
  ],
  "usage": "Provide image_paths array with images to analyze. Configure detection parameters like sensitivity and categories as needed.",
  "output_details": {
    "overall_violence_score": {
      "type": "number",
      "description": "Overall violence score across all images (0.0-1.0)"
    },
    "image_scores": {
      "type": "array",
      "items": {
        "type": "number"
      },
      "description": "Violence score for each input image"
    },
    "detected_categories": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Violence categories detected across all images"
    },
    "high_risk_images": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Paths to images with highest violence scores"
    },
    "detection_details": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Detailed descriptions of detected violent content"
    }
  }
}