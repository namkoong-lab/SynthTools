{
  "tool_name": "Profanity Text Checker",
  "tool_description": "Analyzes extracted text for profanity, hate speech, harassment, and other inappropriate language content.",
  "parameters": {
    "text_content": {
      "type": "string",
      "required": true,
      "description": "Text content to analyze for inappropriate language"
    },
    "language": {
      "type": "string",
      "required": false,
      "description": "Primary language of the text (ISO 639-1 code)",
      "default": "en"
    },
    "check_categories": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "required": false,
      "description": "Categories to check: profanity, hate_speech, harassment, threats, spam",
      "default": [
        "profanity",
        "hate_speech",
        "harassment",
        "threats"
      ]
    },
    "severity_threshold": {
      "type": "string",
      "required": false,
      "description": "Minimum severity level to flag: mild, moderate, severe",
      "default": "moderate"
    }
  },
  "error_messages": [
    "Empty text: text_content cannot be empty or whitespace only.",
    "Unsupported language: The specified language code is not supported for text analysis.",
    "Invalid categories: check_categories must contain only supported category types.",
    "Invalid severity: severity_threshold must be one of: mild, moderate, severe.",
    "Text analysis failed: Unable to process text content for inappropriate language detection."
  ],
  "usage": "Provide text_content extracted from images and configure which types of inappropriate content to detect. Adjust severity_threshold based on your content policies.",
  "output_details": {
    "is_inappropriate": {
      "type": "boolean",
      "description": "Whether inappropriate content was detected above threshold"
    },
    "detected_issues": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "List of specific inappropriate content types found"
    },
    "severity_score": {
      "type": "number",
      "description": "Overall severity score from 0.0 (clean) to 1.0 (highly inappropriate)"
    },
    "flagged_phrases": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Specific phrases or words that triggered the flags"
    }
  }
}