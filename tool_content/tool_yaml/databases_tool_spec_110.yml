field_name: databases
subfield: Database Migration and Data Transfer
task: Data transformation and format conversion during migration process
tool_description: |-
  **STEP 1 — Rate task difficulty**

  This task is **hard**. Data transformation and format conversion during migration involves complex schema mapping, handling diverse data types and formats, ensuring data integrity across different database systems, managing large-scale data processing, and coordinating multiple interdependent transformation steps with high risk of data loss or corruption.

  **STEP 2 — Set a tool budget**

  Given the hard difficulty rating, I'll target **17 tools** to cover the comprehensive workflow of data transformation and format conversion during database migration.

  **STEP 3 — List all tool names and dependencies**

  Tools and their data flow:
  1. **Schema Analyzer** → produces schema metadata
  2. **Data Type Mapper** → consumes source/target schemas, produces type mappings
  3. **Migration Plan Generator** → consumes schemas and mappings, produces migration strategy
  4. **Data Extractor** → produces raw source data
  5. **Data Validator** → consumes data, produces validation reports
  6. **Format Converter** → consumes raw data, produces converted data
  7. **Schema Transformer** → consumes schema mappings, produces transformed schema
  8. **Data Cleaner** → consumes raw data, produces cleaned data
  9. **Relationship Mapper** → consumes schemas, produces relationship mappings
  10. **Batch Processor** → consumes data and transformations, produces processed batches
  11. **Constraint Validator** → consumes data and constraints, produces validation results
  12. **Data Loader** → consumes transformed data, produces load results
  13. **Migration Monitor** → consumes process metrics, produces status reports
  14. **Rollback Manager** → consumes migration state, produces rollback plans
  15. **Performance Optimizer** → consumes query patterns, produces optimization suggestions
  16. **Data Reconciler** → consumes source/target data, produces reconciliation report
  17. **Migration Reporter** → consumes all process data, produces comprehensive reports

  **STEP 4 — Multi-tool plans**

  **Simple plans:**
  - Basic data export: Data Extractor → Format Converter → Data Loader
  - Schema analysis: Schema Analyzer → Data Type Mapper → Schema Transformer

  **Medium plans:**
  - Validated migration: Schema Analyzer → Data Type Mapper → Data Extractor → Data Validator → Format Converter → Data Loader → Data Reconciler
  - Batch processing: Migration Plan Generator → Batch Processor → Constraint Validator → Data Loader → Migration Monitor

  **Complex plans:**
  - Full migration with monitoring: Schema Analyzer → Data Type Mapper → Relationship Mapper → Migration Plan Generator → Data Extractor → Data Cleaner → Format Converter → Batch Processor → Constraint Validator → Data Loader → Migration Monitor → Data Reconciler → Migration Reporter
  - Enterprise migration with rollback: Schema Analyzer → Data Type Mapper → Migration Plan Generator → Performance Optimizer → Data Extractor → Data Validator → Format Converter → Schema Transformer → Data Loader → Migration Monitor → Rollback Manager → Data Reconciler → Migration Reporter

  **STEP 5 — Produce tools**

  ```json
  {
    "tool_name": "Schema Analyzer",
    "tool_description": "Analyzes database schemas to extract metadata including tables, columns, data types, constraints, and relationships for migration planning.",
    "parameters": {
      "connection_string": {
        "type": "string",
        "required": true,
        "description": "Database connection string for the source database"
      },
      "database_type": {
        "type": "string",
        "required": true,
        "description": "Type of database system (mysql, postgresql, oracle, sqlserver, mongodb)"
      },
      "include_views": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to include database views in the analysis"
      }
    },
    "error_messages": [
      "Connection failed: Unable to connect to database. Verify connection string and database accessibility.",
      "Invalid database type: Supported types are mysql, postgresql, oracle, sqlserver, mongodb.",
      "Insufficient permissions: Database user lacks required permissions to read schema metadata.",
      "Schema analysis timeout: Database took too long to respond. Try analyzing smaller schema subsets."
    ],
    "usage": "Provide connection_string and database_type to analyze the source database schema. Set include_views to true if views should be included in the analysis.",
    "output_details": {
      "schema_name": {
        "type": "string",
        "description": "Name of the analyzed database schema"
      },
      "tables": {
        "type": "array",
        "items": {"type": "string"},
        "description": "List of table names in the schema"
      },
      "columns_per_table": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Column definitions for each table"
      },
      "constraints": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Database constraints including primary keys, foreign keys, and unique constraints"
      },
      "total_tables": {
        "type": "integer",
        "description": "Total number of tables analyzed"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Data Type Mapper",
    "tool_description": "Maps data types between different database systems, handling type compatibility and conversion requirements during migration.",
    "parameters": {
      "source_schema": {
        "type": "string",
        "required": true,
        "description": "JSON string containing source database schema information"
      },
      "target_database_type": {
        "type": "string",
        "required": true,
        "description": "Target database system type (mysql, postgresql, oracle, sqlserver, mongodb)"
      },
      "preserve_precision": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to preserve numeric precision during type mapping"
      },
      "allow_lossy_conversion": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to allow potentially lossy data type conversions"
      },
      "custom_mappings": {
        "type": "array",
        "items": {"type": "string"},
        "required": false,
        "default": [],
        "description": "Custom type mapping rules in format 'source_type:target_type'"
      }
    },
    "error_messages": [
      "Invalid schema format: Source schema must be valid JSON with required fields.",
      "Unsupported target database: Target database type must be one of the supported systems.",
      "Incompatible type mapping: Some source types cannot be mapped to target system without data loss.",
      "Invalid custom mapping format: Custom mappings must follow 'source_type:target_type' format.",
      "Precision loss detected: Enable allow_lossy_conversion to proceed with mappings that may lose precision."
    ],
    "usage": "Provide source_schema JSON and target_database_type. Configure precision handling and custom mappings as needed. Review mapping warnings for potential data issues.",
    "output_details": {
      "mapping_rules": {
        "type": "array",
        "items": {"type": "string"},
        "description": "List of data type mapping rules applied"
      },
      "conversion_warnings": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Warnings about potential data loss or conversion issues"
      },
      "incompatible_types": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Data types that cannot be automatically mapped"
      },
      "mapping_success": {
        "type": "boolean",
        "description": "Whether all types were successfully mapped"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Migration Plan Generator",
    "tool_description": "Creates comprehensive migration execution plans including dependency ordering, batch sizing, and rollback strategies.",
    "parameters": {
      "source_schema": {
        "type": "string",
        "required": true,
        "description": "JSON string containing source database schema"
      },
      "target_schema": {
        "type": "string",
        "required": true,
        "description": "JSON string containing target database schema"
      },
      "type_mappings": {
        "type": "string",
        "required": true,
        "description": "JSON string containing data type mappings"
      },
      "batch_size": {
        "type": "integer",
        "required": false,
        "default": 1000,
        "description": "Number of records to process per batch"
      },
      "parallel_threads": {
        "type": "integer",
        "required": false,
        "default": 4,
        "description": "Number of parallel processing threads"
      },
      "migration_strategy": {
        "type": "string",
        "required": false,
        "default": "incremental",
        "description": "Migration approach: full, incremental, or hybrid"
      },
      "include_rollback": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to include rollback procedures in the plan"
      }
    },
    "error_messages": [
      "Invalid schema data: Source or target schema JSON is malformed or missing required fields.",
      "Missing type mappings: Type mappings JSON is required and must contain valid mapping data.",
      "Invalid batch size: Batch size must be a positive integer between 1 and 100000.",
      "Invalid thread count: Parallel threads must be between 1 and 32.",
      "Unsupported strategy: Migration strategy must be 'full', 'incremental', or 'hybrid'.",
      "Circular dependencies: Database schema contains circular foreign key dependencies that prevent ordering."
    ],
    "usage": "Provide source_schema, target_schema, and type_mappings as JSON strings. Configure batch_size and parallel_threads based on system capacity. Choose appropriate migration_strategy for your use case.",
    "output_details": {
      "execution_order": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Ordered list of tables for migration execution"
      },
      "batch_configuration": {
        "type": "string",
        "description": "Batch processing configuration details"
      },
      "estimated_duration": {
        "type": "string",
        "description": "Estimated migration completion time"
      },
      "rollback_strategy": {
        "type": "string",
        "description": "Rollback procedure description"
      },
      "resource_requirements": {
        "type": "string",
        "description": "Estimated system resource requirements"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Data Extractor",
    "tool_description": "Extracts data from source databases with support for filtering, partitioning, and incremental extraction.",
    "parameters": {
      "connection_string": {
        "type": "string",
        "required": true,
        "description": "Source database connection string"
      },
      "table_name": {
        "type": "string",
        "required": true,
        "description": "Name of the table to extract data from"
      },
      "batch_size": {
        "type": "integer",
        "required": false,
        "default": 1000,
        "description": "Number of records to extract per batch"
      },
      "where_clause": {
        "type": "string",
        "required": false,
        "default": "",
        "description": "SQL WHERE clause for filtering data"
      },
      "start_timestamp": {
        "type": "string",
        "required": false,
        "default": "",
        "format": "date-time",
        "description": "Start timestamp for incremental extraction"
      }
    },
    "error_messages": [
      "Database connection failed: Unable to connect to source database with provided connection string.",
      "Table not found: Specified table does not exist in the source database.",
      "Invalid batch size: Batch size must be a positive integer between 1 and 50000.",
      "SQL syntax error: WHERE clause contains invalid SQL syntax.",
      "Invalid timestamp format: Start timestamp must be in ISO 8601 format.",
      "Extraction timeout: Query execution exceeded maximum allowed time limit."
    ],
    "usage": "Provide connection_string and table_name to extract data. Use batch_size to control memory usage. Apply where_clause for filtering and start_timestamp for incremental extraction.",
    "output_details": {
      "extracted_records": {
        "type": "integer",
        "description": "Total number of records extracted"
      },
      "extraction_time": {
        "type": "string",
        "description": "Time taken to complete extraction"
      },
      "data_format": {
        "type": "string",
        "description": "Format of extracted data"
      },
      "last_extracted_timestamp": {
        "type": "string",
        "description": "Timestamp of last extracted record for incremental tracking"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Data Validator",
    "tool_description": "Validates extracted data for completeness, consistency, and quality issues before transformation.",
    "parameters": {
      "data_source": {
        "type": "string",
        "required": true,
        "description": "Path or identifier for the data to validate"
      },
      "schema_definition": {
        "type": "string",
        "required": true,
        "description": "JSON string defining expected data schema"
      },
      "validation_rules": {
        "type": "array",
        "items": {"type": "string"},
        "required": false,
        "default": [],
        "description": "Custom validation rules to apply"
      },
      "sample_size": {
        "type": "integer",
        "required": false,
        "default": 10000,
        "description": "Number of records to sample for validation"
      },
      "null_tolerance": {
        "type": "number",
        "required": false,
        "default": 0.05,
        "description": "Maximum acceptable percentage of null values (0-1)"
      },
      "duplicate_check": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to check for duplicate records"
      },
      "data_profiling": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to perform detailed data profiling"
      }
    },
    "error_messages": [
      "Data source not found: Unable to access specified data source for validation.",
      "Invalid schema definition: Schema JSON is malformed or missing required fields.",
      "Invalid validation rules: Custom validation rules contain syntax errors.",
      "Sample size out of range: Sample size must be between 100 and 1000000.",
      "Invalid null tolerance: Null tolerance must be between 0 and 1.",
      "Validation failed: Data does not meet quality thresholds for migration."
    ],
    "usage": "Provide data_source and schema_definition to validate data quality. Configure validation parameters and rules based on data quality requirements. Review validation results before proceeding with transformation.",
    "output_details": {
      "validation_passed": {
        "type": "boolean",
        "description": "Whether data passed all validation checks"
      },
      "quality_score": {
        "type": "number",
        "description": "Overall data quality score (0-1)"
      },
      "null_percentage": {
        "type": "number",
        "description": "Percentage of null values found"
      },
      "duplicate_count": {
        "type": "integer",
        "description": "Number of duplicate records identified"
      },
      "validation_errors": {
        "type": "array",
        "items": {"type": "string"},
        "description": "List of validation errors found"
      },
      "data_profile": {
        "type": "string",
        "description": "Detailed data profiling results if requested"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Format Converter",
    "tool_description": "Converts data between different formats and encodings while preserving data integrity during migration.",
    "parameters": {
      "input_data": {
        "type": "string",
        "required": true,
        "description": "Path or identifier for input data"
      },
      "source_format": {
        "type": "string",
        "required": true,
        "description": "Source data format (csv, json, xml, parquet, avro, sql)"
      },
      "target_format": {
        "type": "string",
        "required": true,
        "description": "Target data format (csv, json, xml, parquet, avro, sql)"
      },
      "encoding": {
        "type": "string",
        "required": false,
        "default": "utf-8",
        "description": "Character encoding for text formats"
      },
      "delimiter": {
        "type": "string",
        "required": false,
        "default": ",",
        "description": "Field delimiter for CSV format"
      },
      "date_format": {
        "type": "string",
        "required": false,
        "default": "iso",
        "description": "Date format handling (iso, custom, preserve)"
      },
      "null_representation": {
        "type": "string",
        "required": false,
        "default": "null",
        "description": "How to represent null values in target format"
      },
      "compression": {
        "type": "string",
        "required": false,
        "default": "none",
        "description": "Compression method (none, gzip, zip, lz4)"
      },
      "batch_processing": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to process data in batches"
      },
      "validate_conversion": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to validate converted data"
      }
    },
    "error_messages": [
      "Input data not found: Unable to locate or access the specified input data source.",
      "Unsupported format: Source or target format is not supported by the converter.",
      "Encoding error: Unable to decode input data with specified encoding.",
      "Format conversion failed: Data structure is incompatible with target format.",
      "Invalid delimiter: Delimiter must be a single character for CSV format.",
      "Compression error: Unable to apply specified compression method.",
      "Validation failed: Converted data does not match source data integrity checks."
    ],
    "usage": "Specify input_data, source_format, and target_format for basic conversion. Configure encoding, delimiter, and other format-specific parameters as needed. Enable validation to ensure conversion accuracy.",
    "output_details": {
      "conversion_success": {
        "type": "boolean",
        "description": "Whether format conversion completed successfully"
      },
      "output_location": {
        "type": "string",
        "description": "Location of converted data"
      },
      "records_processed": {
        "type": "integer",
        "description": "Number of records converted"
      },
      "conversion_time": {
        "type": "string",
        "description": "Time taken for conversion"
      },
      "file_size_bytes": {
        "type": "integer",
        "description": "Size of converted data file in bytes"
      },
      "conversion_warnings": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Warnings about data conversion issues"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Schema Transformer",
    "tool_description": "Applies schema transformations including column renaming, data type changes, and structural modifications for target database compatibility.",
    "parameters": {
      "source_schema": {
        "type": "string",
        "required": true,
        "description": "JSON string containing source schema definition"
      },
      "transformation_rules": {
        "type": "array",
        "items": {"type": "string"},
        "required": true,
        "description": "List of transformation rules to apply"
      },
      "target_database_type": {
        "type": "string",
        "required": true,
        "description": "Target database system type"
      },
      "preserve_constraints": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to preserve database constraints during transformation"
      },
      "auto_generate_indexes": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to automatically generate recommended indexes"
      }
    },
    "error_messages": [
      "Invalid source schema: Source schema JSON is malformed or incomplete.",
      "Invalid transformation rules: One or more transformation rules have incorrect syntax.",
      "Unsupported target database: Target database type is not supported.",
      "Constraint conflict: Transformation rules conflict with database constraints.",
      "Schema transformation failed: Unable to apply transformations to source schema."
    ],
    "usage": "Provide source_schema as JSON and transformation_rules array. Specify target_database_type for compatibility checks. Configure constraint and index options based on migration requirements.",
    "output_details": {
      "transformed_schema": {
        "type": "string",
        "description": "JSON string of transformed schema"
      },
      "applied_transformations": {
        "type": "array",
        "items": {"type": "string"},
        "description": "List of successfully applied transformations"
      },
      "transformation_warnings": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Warnings about potential issues with transformations"
      },
      "generated_indexes": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Automatically generated index definitions"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Data Cleaner",
    "tool_description": "Cleans and standardizes data by handling missing values, duplicates, and data quality issues during migration preparation.",
    "parameters": {
      "input_data": {
        "type": "string",
        "required": true,
        "description": "Path or identifier for data to be cleaned"
      },
      "cleaning_operations": {
        "type": "array",
        "items": {"type": "string"},
        "required": true,
        "description": "List of cleaning operations to perform"
      },
      "null_handling": {
        "type": "string",
        "required": false,
        "default": "preserve",
        "description": "How to handle null values (preserve, remove, impute, default)"
      },
      "duplicate_strategy": {
        "type": "string",
        "required": false,
        "default": "keep_first",
        "description": "Strategy for handling duplicates (keep_first, keep_last, remove_all)"
      },
      "text_normalization": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to normalize text data (trim, case, encoding)"
      },
      "outlier_detection": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to detect and handle statistical outliers"
      },
      "validation_threshold": {
        "type": "number",
        "required": false,
        "default": 0.95,
        "description": "Minimum data quality threshold (0-1) after cleaning"
      }
    },
    "error_messages": [
      "Input data not accessible: Unable to read data from specified source.",
      "Invalid cleaning operations: One or more cleaning operations are not recognized.",
      "Invalid null handling strategy: Null handling must be one of preserve, remove, impute, default.",
      "Invalid duplicate strategy: Duplicate strategy must be one of keep_first, keep_last, remove_all.",
      "Validation threshold out of range: Threshold must be between 0 and 1.",
      "Cleaning failed: Data quality after cleaning is below validation threshold."
    ],
    "usage": "Specify input_data and cleaning_operations to perform data cleaning. Configure handling strategies for nulls, duplicates, and other data quality issues. Set validation_threshold to ensure cleaned data meets quality requirements.",
    "output_details": {
      "cleaning_success": {
        "type": "boolean",
        "description": "Whether data cleaning completed successfully"
      },
      "records_cleaned": {
        "type": "integer",
        "description": "Number of records processed during cleaning"
      },
      "duplicates_removed": {
        "type": "integer",
        "description": "Number of duplicate records removed"
      },
      "nulls_handled": {
        "type": "integer",
        "description": "Number of null values processed"
      },
      "quality_improvement": {
        "type": "number",
        "description": "Data quality score improvement (0-1)"
      },
      "cleaning_report": {
        "type": "string",
        "description": "Detailed report of cleaning operations performed"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Relationship Mapper",
    "tool_description": "Maps and validates foreign key relationships and referential integrity constraints between source and target database schemas.",
    "parameters": {
      "source_schema": {
        "type": "string",
        "required": true,
        "description": "JSON string containing source database schema with relationships"
      },
      "target_schema": {
        "type": "string",
        "required": true,
        "description": "JSON string containing target database schema"
      },
      "preserve_referential_integrity": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to preserve referential integrity during mapping"
      },
      "cascade_operations": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to include cascade delete/update operations"
      }
    },
    "error_messages": [
      "Invalid source schema: Source schema JSON is missing relationship information.",
      "Invalid target schema: Target schema JSON is malformed or incomplete.",
      "Referential integrity violation: Relationships cannot be preserved in target schema.",
      "Circular dependency detected: Schema contains circular foreign key references.",
      "Orphaned relationships: Some foreign key relationships cannot be mapped to target schema."
    ],
    "usage": "Provide source_schema and target_schema as JSON strings with relationship definitions. Enable preserve_referential_integrity to maintain data consistency. Configure cascade_operations based on business requirements.",
    "output_details": {
      "relationship_mappings": {
        "type": "array",
        "items": {"type": "string"},
        "description": "List of mapped relationships between schemas"
      },
      "integrity_preserved": {
        "type": "boolean",
        "description": "Whether referential integrity can be maintained"
      },
      "orphaned_records": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Relationships that will result in orphaned records"
      },
      "cascade_rules": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Generated cascade operation rules"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Batch Processor",
    "tool_description": "Processes data in configurable batches for efficient memory usage and parallel processing during large-scale migrations.",
    "parameters": {
      "input_data_source": {
        "type": "string",
        "required": true,
        "description": "Source data location for batch processing"
      },
      "batch_size": {
        "type": "integer",
        "required": true,
        "description": "Number of records per batch"
      },
      "processing_operations": {
        "type": "array",
        "items": {"type": "string"},
        "required": true,
        "description": "List of operations to apply to each batch"
      },
      "parallel_workers": {
        "type": "integer",
        "required": false,
        "default": 4,
        "description": "Number of parallel processing workers"
      },
      "memory_limit_mb": {
        "type": "integer",
        "required": false,
        "default": 512,
        "description": "Memory limit per batch in megabytes"
      },
      "error_handling": {
        "type": "string",
        "required": false,
        "default": "skip_batch",
        "description": "Error handling strategy (skip_batch, retry, abort)"
      },
      "checkpoint_frequency": {
        "type": "integer",
        "required": false,
        "default": 10,
        "description": "Number of batches between progress checkpoints"
      },
      "output_format": {
        "type": "string",
        "required": false,
        "default": "same",
        "description": "Output format for processed batches"
      },
      "compression_enabled": {
        "type": "boolean",
        "required": false,
        "default": false,
        "description": "Whether to compress batch output"
      },
      "progress_reporting": {
        "type": "boolean",
        "required": false,
        "default": true,
        "description": "Whether to report processing progress"
      }
    },
    "error_messages": [
      "Data source not found: Unable to access input data source for batch processing.",
      "Invalid batch size: Batch size must be between 1 and 100000.",
      "Invalid processing operations: One or more operations are not recognized.",
      "Invalid worker count: Parallel workers must be between 1 and 32.",
      "Memory limit exceeded: Batch processing requires more memory than allocated limit.",
      "Invalid error handling strategy: Must be one of skip_batch, retry, abort.",
      "Processing failed: Too many batch errors exceeded failure threshold."
    ],
    "usage": "Specify input_data_source, batch_size, and processing_operations for batch processing. Configure parallel_workers and memory_limit_mb based on system resources. Set error_handling strategy and checkpoint_frequency for reliability.",
    "output_details": {
      "total_batches_processed": {
        "type": "integer",
        "description": "Total number of batches successfully processed"
      },
      "records_processed": {
        "type": "integer",
        "description": "Total number of records processed across all batches"
      },
      "processing_time": {
        "type": "string",
        "description": "Total time taken for batch processing"
      },
      "failed_batches": {
        "type": "integer",
        "description": "Number of batches that failed processing"
      },
      "average_batch_time": {
        "type": "string",
        "description": "Average processing time per batch"
      },
      "checkpoint_locations": {
        "type": "array",
        "items": {"type": "string"},
        "description": "List of checkpoint locations for resume capability"
      }
    }
  }
  ```

  ```json
  {
    "tool_name": "Constraint Validator",
    "tool_description": "Validates data against database constraints including primary keys, foreign keys, unique constraints, and check constraints before loading.",
    "parameters": {
      "data_source": {
        "type": "string",
        "required": true,
        "description": "Location of data to validate against constraints"
      },
      "constraint_definitions": {
        "type": "array",
        "items": {"type": "string"},
        "required": true,
        "description": "List of constraint definitions to validate against"
      },
      "validation_mode": {
        "type": "string",
        "required": false,
        "default": "strict",
        "description": "Validation strictness level (strict, relaxed, advisory)"
      },
      "foreign_key_check": {
        "type": "boolean",
        "required": false,
        "
