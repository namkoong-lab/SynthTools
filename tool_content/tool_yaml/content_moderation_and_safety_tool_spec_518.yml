field_name: content_moderation_and_safety
subfield: User-Generated Content Screening
task: Automated text content analysis for harmful language detection
tool_description: "**STEP 1 — Rate task difficulty**\n\nThis task is of **medium** difficulty. It involves multiple complex subproblems including harmful language classification, context analysis, severity assessment, and confidence scoring. There are moderate ambiguities around cultural context and evolving language patterns, with significant coordination needed between detection, analysis, and decision components. The impact of errors is high (false positives/negatives affect user experience), and it requires substantial data processing capabilities.\n\n**STEP 2 — Set a tool budget**\n\nGiven the medium difficulty, I'll target **12 tools** to cover the various aspects of content analysis, detection, classification, and reporting needed for comprehensive harmful language detection.\n\n**STEP 3 — Tool names with dependencies and affordances**\n\n1. **Text Preprocessor** - Consumes: raw text → Produces: cleaned, normalized text\n2. **Language Detector** - Consumes: text → Produces: language identification\n3. **Toxicity Classifier** - Consumes: preprocessed text → Produces: toxicity scores/labels\n4. **Hate Speech Detector** - Consumes: text → Produces: hate speech classification\n5. **Profanity Scanner** - Consumes: text → Produces: profanity detection results\n6. **Context Analyzer** - Consumes: text, conversation history → Produces: contextual insights\n7. **Severity Assessor** - Consumes: multiple detection results → Produces: severity rating\n8. **Confidence Scorer** - Consumes: classification results → Produces: confidence metrics\n9. **False Positive Checker** - Consumes: flagged content, context → Produces: false positive probability\n10. **Content Categorizer** - Consumes: text, detection results → Produces: harm categories\n11. **Risk Evaluator** - Consumes: severity, context, user data → Produces: risk assessment\n12. **Report Generator** - Consumes: all analysis results → Produces: comprehensive report\n\n**STEP 4 — Multi-tool plans**\n\n**Simple Plans:**\n- **Basic Toxicity Check**: Text Preprocessor → Toxicity Classifier → Report Generator (screens for basic toxic language)\n- **Quick Profanity Scan**: Text Preprocessor → Profanity Scanner → Severity Assessor (identifies and rates profane content)\n\n**Medium Plans:**\n- **Standard Content Review**: Text Preprocessor → Language Detector → Toxicity Classifier → Hate Speech Detector → Severity Assessor → Report Generator (comprehensive harmful content detection)\n- **Context-Aware Analysis**: Text Preprocessor → Context Analyzer → Toxicity Classifier → False Positive Checker → Risk Evaluator → Report Generator (analyzes content with conversational context)\n\n**Complex Plans:**\n- **Full Moderation Pipeline**: Text Preprocessor → Language Detector → Toxicity Classifier → Hate Speech Detector → Profanity Scanner → Context Analyzer → Content Categorizer → Severity Assessor → Confidence Scorer → Risk Evaluator → Report Generator (complete analysis workflow)\n- **Advanced Risk Assessment**: Text Preprocessor → Context Analyzer → Toxicity Classifier → Hate Speech Detector → False Positive Checker → Content Categorizer → Severity Assessor → Confidence Scorer → Risk Evaluator → Report Generator (focuses on nuanced risk evaluation with false positive mitigation)\n\n**STEP 5 — Produce tools**\n\n```json\n{\n  \"tool_name\": \"Text Preprocessor\",\n  \"tool_description\": \"Cleans and normalizes text content by removing noise, handling encoding issues, and standardizing format for analysis.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Raw text content to be preprocessed\"\n    },\n    \"remove_urls\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether to remove URLs from text\",\n      \"default\": true\n    },\n    \"normalize_whitespace\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether to normalize whitespace characters\",\n      \"default\": true\n    }\n  },\n  \"error_messages\": [\n    \"Empty\
  \ text content: Provide non-empty text content for processing.\",\n    \"Text too long: Maximum text length is 50,000 characters.\",\n    \"Invalid encoding: Text contains unsupported character encoding.\"\n  ],\n  \"usage\": \"Provide text_content as the primary input. Optionally configure URL removal and whitespace normalization. Use this as the first step before other analysis tools.\",\n  \"output_details\": {\n    \"cleaned_text\": {\n      \"type\": \"string\",\n      \"description\": \"Preprocessed and cleaned text content\"\n    },\n    \"original_length\": {\n      \"type\": \"integer\",\n      \"description\": \"Character count of original text\"\n    },\n    \"processed_length\": {\n      \"type\": \"integer\", \n      \"description\": \"Character count of processed text\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Language Detector\",\n  \"tool_description\": \"Identifies the primary language of text content to enable language-specific analysis and filtering.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Text content for language identification\"\n    },\n    \"min_confidence\": {\n      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Minimum confidence threshold (0.0-1.0)\",\n      \"default\": 0.8\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content: Provide non-empty text for language detection.\",\n    \"Text too short: Minimum 3 characters required for reliable detection.\",\n    \"Invalid confidence threshold: Value must be between 0.0 and 1.0.\"\n  ],\n  \"usage\": \"Input text_content for analysis. Set min_confidence to control detection reliability. Use results to determine if language-specific models should be applied.\",\n  \"output_details\": {\n    \"primary_language\": {\n      \"type\": \"string\",\n      \"description\": \"ISO 639-1 language code of detected primary language\"\n    },\n    \"confidence_score\": {\n      \"type\": \"number\",\n      \"description\": \"Confidence score for language detection (0.0-1.0)\"\n    },\n    \"supported_for_analysis\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether the detected language is supported for content analysis\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Toxicity Classifier\",\n  \"tool_description\": \"Classifies text content for various forms of toxic language including threats, insults, and aggressive behavior using machine learning models.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Text content to analyze for toxicity\"\n    },\n    \"language_code\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"ISO 639-1 language code for language-specific analysis\",\n      \"default\": \"en\"\n    },\n    \"toxicity_threshold\": {\n      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Threshold for toxicity classification (0.0-1.0)\",\n      \"default\": 0.7\n    },\n    \"include_subcategories\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether to include detailed toxicity subcategory scores\",\n      \"default\": true\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content: Provide non-empty text for toxicity analysis.\",\n    \"Unsupported language: Language code not supported for toxicity detection.\",\n    \"Invalid threshold: Toxicity threshold must be between 0.0 and 1.0.\",\n    \"Text too long: Maximum supported text length is 10,000 characters.\"\n  ],\n  \"usage\": \"Input text_content with optional language_code. Adjust toxicity_threshold to control sensitivity. Enable include_subcategories for detailed analysis.\",\n  \"output_details\": {\n    \"is_toxic\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether content is classified as toxic\"\n    },\n    \"toxicity_score\": {\n      \"type\": \"number\",\n      \"description\": \"Overall toxicity score (0.0-1.0)\"\
  \n    },\n    \"threat_score\": {\n      \"type\": \"number\",\n      \"description\": \"Score for threatening language (0.0-1.0)\"\n    },\n    \"insult_score\": {\n      \"type\": \"number\",\n      \"description\": \"Score for insulting language (0.0-1.0)\"\n    },\n    \"identity_attack_score\": {\n      \"type\": \"number\",\n      \"description\": \"Score for identity-based attacks (0.0-1.0)\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Hate Speech Detector\",\n  \"tool_description\": \"Detects hate speech targeting individuals or groups based on protected characteristics using specialized classification models.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Text content to analyze for hate speech\"\n    },\n    \"target_groups\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"required\": false,\n      \"description\": \"Specific target groups to focus detection on\",\n      \"default\": []\n    },\n    \"sensitivity_level\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Detection sensitivity: low, medium, high\",\n      \"default\": \"medium\"\n    },\n    \"include_context_analysis\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether to include contextual analysis for borderline cases\",\n      \"default\": true\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content: Provide non-empty text for hate speech detection.\",\n    \"Invalid sensitivity level: Use low, medium, or high.\",\n    \"Unsupported target group: One or more specified target groups are not supported.\",\n    \"Analysis timeout: Content too complex for processing within time limits.\"\n  ],\n  \"usage\": \"Provide text_content for analysis. Optionally specify target_groups for focused detection. Set sensitivity_level based on moderation requirements.\",\n  \"output_details\": {\n    \"is_hate_speech\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether content contains hate speech\"\n    },\n    \"hate_score\": {\n      \"type\": \"number\",\n      \"description\": \"Hate speech confidence score (0.0-1.0)\"\n    },\n    \"targeted_groups\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"List of groups that appear to be targeted\"\n    },\n    \"hate_categories\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Types of hate speech detected\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Profanity Scanner\",\n  \"tool_description\": \"Scans text for profane language, inappropriate sexual content, and vulgar expressions with customizable filtering levels.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Text content to scan for profanity\"\n    },\n    \"filter_level\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Filtering strictness: strict, moderate, lenient\",\n      \"default\": \"moderate\"\n    },\n    \"detect_masked_profanity\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether to detect masked or obfuscated profanity\",\n      \"default\": true\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content: Provide non-empty text for profanity scanning.\",\n    \"Invalid filter level: Use strict, moderate, or lenient.\",\n    \"Text processing error: Unable to process text due to encoding issues.\"\n  ],\n  \"usage\": \"Input text_content for scanning. Choose appropriate filter_level based on audience. Enable detect_masked_profanity to catch obfuscated terms.\",\n  \"output_details\": {\n    \"contains_profanity\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether profane content was detected\"\n    },\n    \"profanity_count\": {\n      \"type\": \"integer\",\n      \"\
  description\": \"Number of profane terms found\"\n    },\n    \"severity_level\": {\n      \"type\": \"string\",\n      \"description\": \"Severity classification of detected profanity\"\n    },\n    \"profanity_categories\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Categories of profanity detected\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Context Analyzer\",\n  \"tool_description\": \"Analyzes conversational context and thread history to better understand potentially harmful content within its situational framework.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Current text content to analyze\"\n    },\n    \"conversation_history\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"required\": false,\n      \"description\": \"Previous messages in conversation thread\",\n      \"maxItems\": 20,\n      \"default\": []\n    },\n    \"user_relationship\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Relationship between users: strangers, friends, family, colleagues\",\n      \"default\": \"strangers\"\n    },\n    \"platform_context\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Platform type: social_media, gaming, professional, educational, dating\",\n      \"default\": \"social_media\"\n    },\n    \"thread_topic\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Main topic or subject of conversation thread\",\n      \"default\": None\n    },\n    \"time_gap_minutes\": {\n      \"type\": \"integer\",\n      \"required\": false,\n      \"description\": \"Time gap since last message in minutes\",\n      \"default\": 0\n    },\n    \"is_public_thread\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether the conversation is in a public forum\",\n      \"default\": true\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content: Provide the current message text for context analysis.\",\n    \"Invalid user relationship: Use strangers, friends, family, or colleagues.\",\n    \"Invalid platform context: Use social_media, gaming, professional, educational, or dating.\",\n    \"Conversation history too long: Maximum 20 previous messages supported.\",\n    \"Invalid time gap: Time gap must be non-negative integer.\",\n    \"Context processing error: Unable to analyze context due to data inconsistencies.\"\n  ],\n  \"usage\": \"Provide text_content as the main input. Include conversation_history for better context understanding. Set user_relationship and platform_context to match the situation. Specify thread_topic and time_gap_minutes for enhanced analysis.\",\n  \"output_details\": {\n    \"context_risk_level\": {\n      \"type\": \"string\",\n      \"description\": \"Risk assessment based on context: low, medium, high\"\n    },\n    \"escalation_detected\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether conversation shows signs of escalation\"\n    },\n    \"sarcasm_likelihood\": {\n      \"type\": \"number\",\n      \"description\": \"Probability that content is sarcastic (0.0-1.0)\"\n    },\n    \"conversation_tone\": {\n      \"type\": \"string\",\n      \"description\": \"Overall tone of conversation thread\"\n    },\n    \"context_factors\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"List of contextual factors influencing analysis\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Severity Assessor\",\n  \"tool_description\": \"Evaluates the overall severity of harmful content by combining multiple detection results and contextual factors.\",\n  \"parameters\": {\n    \"toxicity_score\": {\n      \"type\": \"number\",\n      \"required\": true,\n      \"description\": \"Toxicity classification score (0.0-1.0)\"\n    },\n    \"hate_speech_score\": {\n\
  \      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Hate speech detection score (0.0-1.0)\",\n      \"default\": 0.0\n    },\n    \"profanity_severity\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Profanity severity level: none, mild, moderate, severe\",\n      \"default\": \"none\"\n    },\n    \"context_risk_level\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Context-based risk assessment: low, medium, high\",\n      \"default\": \"low\"\n    },\n    \"target_vulnerability\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Vulnerability of target audience: general, minors, vulnerable_groups\",\n      \"default\": \"general\"\n    }\n  },\n  \"error_messages\": [\n    \"Invalid toxicity score: Score must be between 0.0 and 1.0.\",\n    \"Invalid hate speech score: Score must be between 0.0 and 1.0.\",\n    \"Invalid profanity severity: Use none, mild, moderate, or severe.\",\n    \"Invalid context risk level: Use low, medium, or high.\",\n    \"Invalid target vulnerability: Use general, minors, or vulnerable_groups.\"\n  ],\n  \"usage\": \"Provide toxicity_score as the primary input. Include additional scores and contextual factors for comprehensive severity assessment.\",\n  \"output_details\": {\n    \"overall_severity\": {\n      \"type\": \"string\",\n      \"description\": \"Overall severity classification: low, moderate, high, severe\"\n    },\n    \"severity_score\": {\n      \"type\": \"number\",\n      \"description\": \"Numerical severity score (0.0-1.0)\"\n    },\n    \"recommended_action\": {\n      \"type\": \"string\",\n      \"description\": \"Suggested moderation action based on severity\"\n    },\n    \"priority_level\": {\n      \"type\": \"integer\",\n      \"description\": \"Priority for human review (1-5, higher is more urgent)\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Confidence Scorer\",\n  \"tool_description\": \"Calculates confidence metrics for harmful content detection results by analyzing model certainty and potential edge cases.\",\n  \"parameters\": {\n    \"detection_scores\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"number\"\n      },\n      \"required\": true,\n      \"description\": \"Array of detection scores from different classifiers\",\n      \"minItems\": 1,\n      \"maxItems\": 10\n    },\n    \"model_agreement\": {\n      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Agreement level between different models (0.0-1.0)\",\n      \"default\": None\n    },\n    \"text_ambiguity\": {\n      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Ambiguity level of the analyzed text (0.0-1.0)\",\n      \"default\": None\n    }\n  },\n  \"error_messages\": [\n    \"Empty detection scores: Provide at least one detection score.\",\n    \"Invalid detection score: All scores must be between 0.0 and 1.0.\",\n    \"Invalid model agreement: Agreement score must be between 0.0 and 1.0.\",\n    \"Invalid text ambiguity: Ambiguity score must be between 0.0 and 1.0.\",\n    \"Too many detection scores: Maximum 10 scores supported.\"\n  ],\n  \"usage\": \"Provide detection_scores from various classifiers. Optionally include model_agreement and text_ambiguity for enhanced confidence calculation.\",\n  \"output_details\": {\n    \"overall_confidence\": {\n      \"type\": \"number\",\n      \"description\": \"Overall confidence in detection results (0.0-1.0)\"\n    },\n    \"confidence_level\": {\n      \"type\": \"string\",\n      \"description\": \"Confidence classification: low, medium, high\"\n    },\n    \"uncertainty_factors\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Factors contributing to uncertainty in results\"\n    },\n    \"review_recommended\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether human review is recommended due to low confidence\"\n    }\n  }\n\
  }\n```\n\n```json\n{\n  \"tool_name\": \"False Positive Checker\",\n  \"tool_description\": \"Analyzes flagged content to identify potential false positives by examining context, intent, and linguistic patterns.\",\n  \"parameters\": {\n    \"flagged_text\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Text content that was flagged as potentially harmful\"\n    },\n    \"detection_type\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Type of harmful content detected: toxicity, hate_speech, profanity, threat\"\n    },\n    \"confidence_score\": {\n      \"type\": \"number\",\n      \"required\": true,\n      \"description\": \"Original detection confidence score (0.0-1.0)\"\n    },\n    \"context_data\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Additional context about the content\",\n      \"default\": None\n    },\n    \"user_intent\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Apparent user intent: educational, humorous, news_reporting, creative, other\",\n      \"default\": \"other\"\n    }\n  },\n  \"error_messages\": [\n    \"Empty flagged text: Provide the text content that was flagged.\",\n    \"Invalid detection type: Use toxicity, hate_speech, profanity, or threat.\",\n    \"Invalid confidence score: Score must be between 0.0 and 1.0.\",\n    \"Invalid user intent: Use educational, humorous, news_reporting, creative, or other.\"\n  ],\n  \"usage\": \"Input flagged_text with detection_type and original confidence_score. Provide context_data and user_intent for better false positive detection.\",\n  \"output_details\": {\n    \"false_positive_probability\": {\n      \"type\": \"number\",\n      \"description\": \"Probability that detection is a false positive (0.0-1.0)\"\n    },\n    \"likely_false_positive\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether the detection is likely a false positive\"\n    },\n    \"false_positive_reasons\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Reasons why this might be a false positive\"\n    },\n    \"revised_risk_level\": {\n      \"type\": \"string\",\n      \"description\": \"Revised risk assessment: low, medium, high\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Content Categorizer\",\n  \"tool_description\": \"Categorizes harmful content into specific types and subcategories for targeted moderation and policy enforcement.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Text content to categorize\"\n    },\n    \"is_toxic\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether content is flagged as toxic\",\n      \"default\": false\n    },\n    \"is_hate_speech\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether content contains hate speech\",\n      \"default\": false\n    },\n    \"contains_profanity\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether content contains profanity\",\n      \"default\": false\n    },\n    \"targeted_groups\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"required\": false,\n      \"description\": \"Groups targeted by harmful content\",\n      \"default\": []\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content: Provide text content for categorization.\",\n    \"Categorization failed: Unable to determine appropriate categories for content.\",\n    \"Invalid group specification: One or more targeted groups are not recognized.\"\n  ],\n  \"usage\": \"Provide text_content along with detection flags (is_toxic, is_hate_speech, contains_profanity). Include targeted_groups if applicable for detailed categorization.\",\n  \"output_details\": {\n    \"primary_category\": {\n      \"type\": \"string\",\n      \"description\"\
  : \"Main category of harmful content\"\n    },\n    \"subcategories\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Detailed subcategories of harmful content\"\n    },\n    \"policy_violations\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Specific platform policies violated\"\n    },\n    \"harm_type\": {\n      \"type\": \"string\",\n      \"description\": \"Type of harm: individual, group, societal, platform\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Risk Evaluator\",\n  \"tool_description\": \"Evaluates overall risk posed by content considering severity, user factors, platform context, and potential impact to determine appropriate response measures.\",\n  \"parameters\": {\n    \"severity_score\": {\n      \"type\": \"number\",\n      \"required\": true,\n      \"description\": \"Content severity score from severity assessment (0.0-1.0)\"\n    },\n    \"confidence_level\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Confidence in detection results: low, medium, high\"\n    },\n    \"user_history_risk\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"User's violation history: clean, minor_violations, repeat_offender, severe_violator\",\n      \"default\": \"clean\"\n    },\n    \"audience_size\": {\n      \"type\": \"integer\",\n      \"required\": false,\n      \"description\": \"Potential audience size for the content\",\n      \"default\": 1\n    },\n    \"platform_visibility\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Content visibility: private, limited, public, viral_potential\",\n      \"default\": \"public\"\n    },\n    \"false_positive_probability\": {\n      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Probability of false positive (0.0-1.0)\",\n      \"default\": 0.0\n    },\n    \"target_vulnerability\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Vulnerability of target audience: general, minors, vulnerable_groups\",\n      \"default\": \"general\"\n    },\n    \"escalation_potential\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether content could lead to further escalation\",\n      \"default\": false\n    },\n    \"legal_implications\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether content may have legal implications\",\n      \"default\": false\n    },\n    \"cultural_sensitivity\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Cultural sensitivity level: low, medium, high\",\n      \"default\": \"medium\"\n    },\n    \"time_sensitivity\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Time sensitivity for response: low, medium, high, urgent\",\n      \"default\": \"medium\"\n    }\n  },\n  \"error_messages\": [\n    \"Invalid severity score: Score must be between 0.0 and 1.0.\",\n    \"Invalid confidence level: Use low, medium, or high.\",\n    \"Invalid user history risk: Use clean, minor_violations, repeat_offender, or severe_violator.\",\n    \"Invalid audience size: Must be positive integer.\",\n    \"Invalid platform visibility: Use private, limited, public, or viral_potential.\",\n    \"Invalid false positive probability: Must be between 0.0 and 1.0.\",\n    \"Invalid target vulnerability: Use general, minors, or vulnerable_groups.\",\n    \"Invalid cultural sensitivity: Use low, medium, or high.\",\n    \"Invalid time sensitivity: Use low, medium, high, or urgent.\"\n  ],\n  \"usage\": \"Provide severity_score and confidence_level as required inputs. Include user_history_risk, audience_size, and platform_visibility for comprehensive risk assessment. Set optional parameters based on specific context and requirements.\",\n  \"output_details\": {\n    \"overall_risk_level\": {\n     \
  \ \"type\": \"string\",\n      \"description\": \"Overall risk classification: minimal, low, medium, high, critical\"\n    },\n    \"risk_score\": {\n      \"type\": \"number\",\n      \"description\": \"Numerical risk score (0.0-1.0)\"\n    },\n    \"recommended_actions\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"List of recommended moderation actions\"\n    },\n    \"urgency_level\": {\n      \"type\": \"string\",\n      \"description\": \"Response urgency: routine, priority, urgent, immediate\"\n    },\n    \"escalation_required\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether escalation to human moderators is required\"\n    }\n  }\n}\n```\n\n```json\n{\n  \"tool_name\": \"Report Generator\",\n  \"tool_description\": \"Generates comprehensive analysis reports summarizing all content moderation findings, decisions, and recommendations for stakeholders.\",\n  \"parameters\": {\n    \"text_content\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Original text content that was analyzed\"\n    },\n    \"analysis_results\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"required\": true,\n      \"description\": \"Key findings from various analysis tools\",\n      \"minItems\": 1,\n      \"maxItems\": 20\n    },\n    \"risk_level\": {\n      \"type\": \"string\",\n      \"required\": true,\n      \"description\": \"Overall risk assessment: minimal, low, medium, high, critical\"\n    },\n    \"recommended_actions\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"required\": true,\n      \"description\": \"List of recommended moderation actions\",\n      \"minItems\": 1,\n      \"maxItems\": 10\n    },\n    \"confidence_level\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Overall confidence in analysis: low, medium, high\",\n      \"default\": \"medium\"\n    },\n    \"false_positive_risk\": {\n      \"type\": \"number\",\n      \"required\": false,\n      \"description\": \"Risk of false positive (0.0-1.0)\",\n      \"default\": 0.0\n    },\n    \"escalation_required\": {\n      \"type\": \"boolean\",\n      \"required\": false,\n      \"description\": \"Whether human review escalation is needed\",\n      \"default\": false\n    },\n    \"policy_violations\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"required\": false,\n      \"description\": \"Platform policies violated by content\",\n      \"default\": []\n    },\n    \"report_format\": {\n      \"type\": \"string\",\n      \"required\": false,\n      \"description\": \"Report format type: summary, detailed, technical\",\n      \"default\": \"summary\"\n    }\n  },\n  \"error_messages\": [\n    \"Empty text content"
